

# Setup all Docker containers
docker-compose up

# Install the Redis Kafa Connect connector from HTTP command
curl -s -X POST -H 'Content-Type: application/json' --data @redis-sink-config.json http://localhost:8083/connectors

# Check installation
http://localhost:8083/connectors/redis-sink/status

# Send a typed Kafka message to the target Redis topic
# A Kafka topic is created by default at the first insertion
kafka-console-producer --broker-list localhost:9092 --topic redis


{"payload":{"key":"users:1:username","value":"Greg"},"schema":{"name":"io.github.jaredpetersen.kafkaconnectredis.RedisSetCommand","type":"struct","fields":[{"field":"key","type":"string","optional":false},{"field":"value","type":"string","optional":false},{"field":"expiration","type":"struct","fields":[{"field":"type","type":"string","optional":false},{"field":"time","type":"int64","optional":true}],"optional":true},{"field":"condition","type":"string","optional":true}]}}




# More information here
Lien : https://medium.com/swlh/sinking-and-sourcing-redis-data-with-kafka-connect-redis-9515201e7a1



choisir donné qui change souvent
ecrire en python un injecteur, lit les données de la source puis les envois les donnée a kafka


